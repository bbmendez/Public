{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP Text Mining_Barbara.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS0Zo3xoAmbV",
        "outputId": "26f37a4e-d409-4821-d7b7-d09b5a29f9ed"
      },
      "source": [
        "\"\"\"\n",
        "Main module for the wikitree application.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "! pip install wikipedia\n",
        "import wikipedia\n",
        "from wikipedia.exceptions import DisambiguationError, PageError\n",
        "from tabulate import tabulate\n",
        "from spacy.matcher import Matcher\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "#parser = argparse.ArgumentParser()\n",
        "#parser.add_argument('query', type=str, metavar='Q',\n",
        "#                    help='Query to retrieve as root from Wikipedia.')\n",
        "#parser.add_argument('--depth', '-d', type=int, default=2,\n",
        "#                   help='Max tree depth.')\n",
        "\n"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3cnS96mgl5O"
      },
      "source": [
        "\n",
        "class RelationshipGraph(object):\n",
        "    \"\"\"\n",
        "    This class represents a relationship graph containing multiple nodes and the edges connecting them.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, query: str, depth: int = 2, width: int = 2) -> None:\n",
        "        \"\"\"\n",
        "        Initialize an empty graph with an initial query.\n",
        "        :param query: This is the value for the query to generate the initial node in the graph. The value \n",
        "            will be updated when the node is fetched if there is a disambiguation error or if the name of \n",
        "            the page is different.\n",
        "        \"\"\"\n",
        "        self.initial_query = query\n",
        "        self.nodes = {}  # nodes indexed by key/name\n",
        "        self.edges = []  # set of tuples of three elements, the keys for the nodes at either end of the \n",
        "                         # edge and the label for the relationship\n",
        "        self.depth = depth\n",
        "        self.width = width\n",
        "\n",
        "    def fetch(self):\n",
        "        \"\"\"\n",
        "        Fetch nodes and their relationships from Wikipedia.\n",
        "        \"\"\"\n",
        "        initial_node = GraphNode(self.initial_query)\n",
        "        initial_node.fetch(self, self.depth, self.width)\n",
        "\n",
        "    def display(self):\n",
        "        print(tabulate(\n",
        "            self.edges,\n",
        "            headers=('From', 'To', 'Label')\n",
        "        ))\n",
        "        print('\\n' + '\\n'.join([_.name for _ in self.nodes.values()]))\n",
        "\n",
        "\n",
        "class GraphNode(object):\n",
        "    \"\"\"\n",
        "    This class represents a node in the concept graph.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, query):\n",
        "        \"\"\"\n",
        "        Initialize graph node.\n",
        "        :param query: Query for this node.\n",
        "        \"\"\"\n",
        "        self.query = query\n",
        "        self.page = None\n",
        "        self.name = None\n",
        "\n",
        "    def fetch(self, graph: RelationshipGraph, depth: int = 2, width: int = 2):\n",
        "        \"\"\"\n",
        "        Retrive information for this node in the graph from Wikipedia. Determine candidates for adjacent \n",
        "        nodes and fetch for those as well with depth-1.\n",
        "        :param depth: Depth of search.\n",
        "        \"\"\"\n",
        "        print(f'Fetching: {self.query}')\n",
        "        try:\n",
        "            self.page = wikipedia.page(self.query, auto_suggest=False)\n",
        "        except DisambiguationError as err:\n",
        "            print(f'Disambiguating to {err.args[1][0]}')\n",
        "            self.page = wikipedia.page(err.args[1][0], auto_suggest=False)\n",
        "        self.name = self.page.title\n",
        "\n",
        "        graph.nodes[self.name] = self\n",
        "        \n",
        "        if depth > 0:\n",
        "            # Extract entities\n",
        "            entities = nlp(self.page.content).ents  # Entities extracted from the text\n",
        "            entity_counts = {}\n",
        "\n",
        "            for e in entities:\n",
        "                entity_counts[(e.text, e.label_)] = entity_counts.get((e.text, e.label_), 0) + 1\n",
        "\n",
        "            # Select entities\n",
        "            labels = ('PERSON', )\n",
        "            candidate_entities = [k[0] for k, v in sorted(entity_counts.items(), key=lambda _: _[1]) if k[1] in labels]\n",
        "            selected_entities = []\n",
        "            while candidate_entities and len(selected_entities) < width:\n",
        "                candidate = candidate_entities.pop()\n",
        "                try:\n",
        "                    page = wikipedia.page(candidate, auto_suggest=False)\n",
        "                except DisambiguationError as err:\n",
        "                    page = wikipedia.page(err.args[1][0], auto_suggest=False)\n",
        "                except PageError:\n",
        "                    continue\n",
        "                print(f'{candidate} -> {page.title}')\n",
        "                if page.title != self.page.title and page.title not in graph.nodes:\n",
        "                    selected_entities.append(candidate)\n",
        "\n",
        "            # Get selected entitites\n",
        "            for query in selected_entities:\n",
        "                if query in graph.nodes:\n",
        "                    node = graph.nodes.get(query)\n",
        "                else:\n",
        "                    node = GraphNode(query)\n",
        "                    node.fetch(graph, depth=depth - 1, width=width)\n",
        "                \n",
        "                graph.edges.append((self.name, node.name, 'UNK'))\n",
        "\n",
        "\n"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXxXPhiMhj3a",
        "outputId": "63669b76-30c4-4c6c-eb9a-6707b76d6f14"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    print('Welcome to Wikitree!')\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    graph = RelationshipGraph(\"Lionel Messi\")\n",
        "    graph.fetch()\n",
        "\n",
        "    graph.display()\n"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome to Wikitree!\n",
            "Fetching: Lionel Messi\n",
            "Messi -> Lionel Messi\n",
            "Maradona -> Diego Maradona\n",
            "Cristiano Ronaldo -> Cristiano Ronaldo\n",
            "Fetching: Maradona\n",
            "Maradona -> Diego Maradona\n",
            "Diego Maradona -> Diego Maradona\n",
            "Boca Juniors -> Boca Juniors\n",
            "Diego -> Diego\n",
            "Fetching: Boca Juniors\n",
            "Fetching: Diego\n",
            "Fetching: Cristiano Ronaldo\n",
            "Ronaldo -> Ronaldo\n",
            "Cristiano Ronaldo -> Cristiano Ronaldo\n",
            "Atlético Madrid -> Atlético Madrid\n",
            "Fetching: Ronaldo\n",
            "Fetching: Atlético Madrid\n",
            "From               To                 Label\n",
            "-----------------  -----------------  -------\n",
            "Diego Maradona     Boca Juniors       UNK\n",
            "Diego Maradona     Diego              UNK\n",
            "Lionel Messi       Diego Maradona     UNK\n",
            "Cristiano Ronaldo  Ronaldo            UNK\n",
            "Cristiano Ronaldo  Atlético Madrid    UNK\n",
            "Lionel Messi       Cristiano Ronaldo  UNK\n",
            "\n",
            "Lionel Messi\n",
            "Diego Maradona\n",
            "Boca Juniors\n",
            "Diego\n",
            "Cristiano Ronaldo\n",
            "Ronaldo\n",
            "Atlético Madrid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhLWqbJUkMAm",
        "outputId": "e929cd03-a0c6-40bb-d5b2-23f9796849af"
      },
      "source": [
        "test = RelationshipGraph(\"Lionel Messi\")\n",
        "test.fetch()\n",
        "test.display()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching: Lionel Messi\n",
            "Messi -> Lionel Messi\n",
            "Maradona -> Diego Maradona\n",
            "Cristiano Ronaldo -> Cristiano Ronaldo\n",
            "Fetching: Maradona\n",
            "Maradona -> Diego Maradona\n",
            "Diego Maradona -> Diego Maradona\n",
            "Boca Juniors -> Boca Juniors\n",
            "Diego -> Diego\n",
            "Fetching: Boca Juniors\n",
            "Fetching: Diego\n",
            "Fetching: Cristiano Ronaldo\n",
            "Ronaldo -> Ronaldo\n",
            "Cristiano Ronaldo -> Cristiano Ronaldo\n",
            "Atlético Madrid -> Atlético Madrid\n",
            "Fetching: Ronaldo\n",
            "Fetching: Atlético Madrid\n",
            "From               To                 Label\n",
            "-----------------  -----------------  -------\n",
            "Diego Maradona     Boca Juniors       UNK\n",
            "Diego Maradona     Diego              UNK\n",
            "Lionel Messi       Diego Maradona     UNK\n",
            "Cristiano Ronaldo  Ronaldo            UNK\n",
            "Cristiano Ronaldo  Atlético Madrid    UNK\n",
            "Lionel Messi       Cristiano Ronaldo  UNK\n",
            "\n",
            "Lionel Messi\n",
            "Diego Maradona\n",
            "Boca Juniors\n",
            "Diego\n",
            "Cristiano Ronaldo\n",
            "Ronaldo\n",
            "Atlético Madrid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_L2XhfiGiHB"
      },
      "source": [
        "# Get content again of Wikipedia page\n",
        "try:\n",
        "  pag = wikipedia.page(\"Lionel Messi\", auto_suggest=False).content\n",
        "except DisambiguationError as err:\n",
        "  pag = wikipedia.page(err.args[1][0], auto_suggest=False).content"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elsyvMNVn3xI",
        "outputId": "f7f54086-702e-471c-b022-090d26139513"
      },
      "source": [
        "# Nodes:\n",
        "nodes = test.nodes\n",
        "nodes"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Atlético Madrid': <__main__.GraphNode at 0x7f1ddddfd9d0>,\n",
              " 'Boca Juniors': <__main__.GraphNode at 0x7f1ddde11510>,\n",
              " 'Cristiano Ronaldo': <__main__.GraphNode at 0x7f1ddde24dd0>,\n",
              " 'Diego': <__main__.GraphNode at 0x7f1ddddfd8d0>,\n",
              " 'Diego Maradona': <__main__.GraphNode at 0x7f1dc2172710>,\n",
              " 'Lionel Messi': <__main__.GraphNode at 0x7f1dc212add0>,\n",
              " 'Ronaldo': <__main__.GraphNode at 0x7f1dc11d5290>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6SeTU4No0_o"
      },
      "source": [
        "# Function to get a list of the nodes names\n",
        "def getList(dict):\n",
        "    list = []\n",
        "    for key in dict.keys():\n",
        "        list.append(key)\n",
        "    return list"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICYq8xxZtZCV",
        "outputId": "3e5b0e69-72ed-445e-d7f7-e77cc8c4e591"
      },
      "source": [
        "#List of nodes:\n",
        "List=getList(nodes)\n",
        "List"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lionel Messi',\n",
              " 'Diego Maradona',\n",
              " 'Boca Juniors',\n",
              " 'Diego',\n",
              " 'Cristiano Ronaldo',\n",
              " 'Ronaldo',\n",
              " 'Atlético Madrid']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3KDuwr5q4tI"
      },
      "source": [
        "# Create list with different relationshops between de nodes.\n",
        "# (Obviamente esto hay que hacerlo mejor)\n",
        "Nivel_1_a = [List[0],List[1]]\n",
        "Nivel_1_b = [List[0],List[4]]\n",
        "Nivel_2_a = [List[1],List[2]]\n",
        "Nivel_2_b = [List[1],List[3]]\n",
        "Nivel_2_c = [List[4],List[5]]\n",
        "Nivel_2_d = [List[4],List[6]]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpVg6W5B6_iV"
      },
      "source": [
        "# Function that extracts the ralationship from a sentance\n",
        "# (Sacada del articulo del profesor)\n",
        "def get_relation(sent):\n",
        "\n",
        "  doc = nlp(sent)\n",
        "\n",
        "  # Matcher class object \n",
        "  matcher = Matcher(nlp.vocab)\n",
        "\n",
        "  #define the pattern \n",
        "  pattern = [{'DEP':'ROOT'}, \n",
        "            {'DEP':'prep','OP':\"?\"},\n",
        "            {'DEP':'agent','OP':\"?\"},  \n",
        "            {'POS':'ADJ','OP':\"?\"}] \n",
        "\n",
        "  matcher.add(\"matching_1\", None, pattern) \n",
        "\n",
        "  matches = matcher(doc)\n",
        "  k = len(matches) - 1\n",
        "\n",
        "  span = doc[matches[k][1]:matches[k][2]] \n",
        "\n",
        "  return(span.text)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9fgNv3PKoYQ"
      },
      "source": [
        "# El exto no los nombra con nombre y apellido entonces reemplazo por solo apellidos para probar las funciones:\n",
        "Nivel_1_a = ['Messi', 'Maradona']\n"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc6uRmx7VhFa"
      },
      "source": [
        "#Function that extracts sentences that contain both entities\n",
        "def get_sen_w_words(text,list1):\n",
        "  sentences_with_word = []\n",
        "  for sen in sent_tokenize(text):\n",
        "     l = list(nlp(sen).ents)\n",
        "     l_strings  = [i.text for i in l]\n",
        "     if len(set(l_strings).intersection(list1))>1:\n",
        "        sentences_with_word.append(sen)\n",
        "  return(sentences_with_word)"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5XTtyPZWEsR"
      },
      "source": [
        "# Obtain sentences from the Wikipedia page that have both entities\n",
        "sentences_to_analyze = get_sen_w_words(pag,Nivel_1_a)"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e2j4SVzH-LZ",
        "outputId": "849150b3-39f6-4151-d4a7-f84ab0ba86e9"
      },
      "source": [
        "relations=[]\n",
        "for sen in sentences_to_analyze:\n",
        "  relations.append(get_relation(sen))\n",
        "relations\n"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['proved',\n",
              " 'collected',\n",
              " 'criticised for',\n",
              " 'scored',\n",
              " 'outlined',\n",
              " 'followed in',\n",
              " 'gained greater',\n",
              " 'was',\n",
              " 'said in',\n",
              " 'been',\n",
              " 'echoed',\n",
              " 'held in lesser',\n",
              " 'is in',\n",
              " 'questioned']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    }
  ]
}